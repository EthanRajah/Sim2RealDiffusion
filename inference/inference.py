import torch
from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler
from diffusers.utils import load_image
from controlnet_aux import PidiNetDetector
import numpy as np
from PIL import Image
import argparse
from time import time
import os

def resize_for_condition_image(input_image: Image, resolution: int):
    input_image = input_image.convert("RGB")
    W, H = input_image.size
    k = float(resolution) / min(H, W)
    H *= k
    W *= k
    H = int(round(H / 64.0)) * 64
    W = int(round(W / 64.0)) * 64
    img = input_image.resize((W, H), resample=Image.LANCZOS)
    return img

def controlnet_infer(input_path, model_id, output_dir, prompt):
    """
    Function to run inference on sim2real dreambooth model with tile and softedge control net.
    Supports passing a single image or a directory of images.
    """

    # Load tile and softedge control net models
    tile_control = ControlNetModel.from_pretrained('lllyasviel/control_v11f1e_sd15_tile', torch_dtype=torch.float16)
    softedge_control = ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_softedge', torch_dtype=torch.float16)
    controlnet = [tile_control, softedge_control]
    
    # Apply control net to sim2real model to generate pipeline
    pipe = StableDiffusionControlNetPipeline.from_pretrained(model_id, controlnet=controlnet, torch_dtype=torch.float16).to('cuda')
    generator = torch.Generator(device='cpu').manual_seed(0)
    
    # Reduce inference times by using a multistep scheduler
    pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)
    pipe.enable_xformers_memory_efficient_attention()

    # Check if input_path is a file or directory
    if os.path.isdir(input_path):
        image_paths = [os.path.join(input_path, f) for f in os.listdir(input_path) if f.lower().endswith(('png', 'jpg', 'jpeg'))]
    else:
        image_paths = [input_path]

    # Ensure the output directory exists
    os.makedirs(output_dir, exist_ok=True)

    # Process each image
    start = time()
    for img_path in image_paths:
        img = load_image(img_path)
        tile_condition_img = resize_for_condition_image(img, 512)

        # Prepare edge mask for softedge control net
        processor = PidiNetDetector.from_pretrained('lllyasviel/Annotators')
        edge_condition_image = processor(img, safe=True)

        # Run inference using pipeline
        images = [tile_condition_img, edge_condition_image]
        output_image = pipe(prompt, images, num_inference_steps=20, generator=generator, controlnet_conditioning_scale=[1.0, 1.0], guidance_scale=4.5).images[0]
        
        # Save the output image
        base_name = os.path.splitext(os.path.basename(img_path))[0]
        output_image.save(os.path.join(output_dir, f"{base_name}_gen.png"))

    print(f"Total time taken: {time() - start:.2f} seconds")
    print(f"Average time per image: {(time() - start) / len(image_paths):.2f} seconds")
    print(f"Inference completed. Results saved to {output_dir}")

def generate_latents(input_path, model_id, output_dir, prompt, resolution, guidance=4.5):
    """
    Function to run inference on sim2real dreambooth model with tile and softedge control net
    but only output the latents generated by the model.
    """
     # Load tile and softedge control net models
    tile_control = ControlNetModel.from_pretrained('lllyasviel/control_v11f1e_sd15_tile', torch_dtype=torch.float16)
    softedge_control = ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_softedge', torch_dtype=torch.float16)
    controlnet = [tile_control, softedge_control]
    
    # Apply control net to sim2real model to generate pipeline
    pipe = StableDiffusionControlNetPipeline.from_pretrained(model_id, controlnet=controlnet, torch_dtype=torch.float16).to('cuda')
    # Reduce inference times by using a multistep scheduler
    pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)
    pipe.scheduler.set_timesteps(20)
    pipe.enable_xformers_memory_efficient_attention()
    generator = torch.Generator(device='cuda').manual_seed(0)

    # Check if input_path is a file or directory
    if os.path.isdir(input_path):
        image_paths = [os.path.join(input_path, f) for f in os.listdir(input_path) if f.lower().endswith(('png', 'jpg', 'jpeg'))]
    else:
        image_paths = [input_path]
    # Ensure the output directory exists
    os.makedirs(output_dir, exist_ok=True)
    # Process each image
    start = time()
    for img_path in image_paths:
        img = load_image(img_path)
        tile_condition_img = resize_for_condition_image(img, resolution)
        # Prepare edge mask for softedge control net
        processor = PidiNetDetector.from_pretrained('lllyasviel/Annotators')
        edge_condition_image = processor(img, safe=True)
        conditioning_images = [tile_condition_img, edge_condition_image]

        # Encode the prompt
        text_input = pipe.tokenizer(prompt, return_tensors="pt").input_ids.to(pipe.device)
        text_embeddings = pipe.text_encoder(text_input)[0]
        # Encode unconditional input for guidance
        uncond_input = pipe.tokenizer([""], return_tensors="pt").input_ids.to(pipe.device)
        uncond_embeddings = pipe.text_encoder(uncond_input)[0]
        # Combine embeddings
        # Ensure both embeddings match in sequence length
        uncond_embeddings = uncond_embeddings.repeat(1, text_embeddings.shape[1] // uncond_embeddings.shape[1], 1)
        text_embeddings = torch.cat([uncond_embeddings, text_embeddings])
        # Initialize latents
        latents = torch.randn(
            (1, pipe.unet.in_channels, resolution // 8, resolution // 8),
            generator=generator,
            device=pipe.device,
            dtype=torch.float16,
        )
        latents = latents * pipe.scheduler.init_noise_sigma

        # Perform denoising with ControlNet conditioning
        for t in pipe.scheduler.timesteps:
            # Expand the latents if we are doing classifier-free guidance to avoid doing two forward passes
            latent_model_input = torch.cat([latents] * 2)
            latent_model_input = pipe.scheduler.scale_model_input(latent_model_input, t)
            # Combine with controlnet conditioning
            for i, controlnet_model in enumerate(pipe.controlnet.nets):
                # Process each conditioning image with the corresponding ControlNet model
                conditioning_image = conditioning_images[i]
                control_result = controlnet_model(conditioning_image, t, latent_model_input, text_embeddings).sample
                latent_model_input += sum(control_result)
            # Predict noise
            noise_pred = pipe.unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample
            # Apply guidance
            noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)
            noise_pred = noise_pred_uncond + guidance * (noise_pred_text - noise_pred_uncond)
            # Update latents for previous noisy sample based on prediction
            latents = pipe.scheduler.step(noise_pred, t, latents).prev_sample

        # Save latents instead of decoding
        base_name = os.path.splitext(os.path.basename(img_path))[0]
        latent_file = os.path.join(output_dir, f"{base_name}_latents.pt")
        torch.save(latents, latent_file)

    print(f"Total time taken: {time() - start:.2f} seconds")
    print(f"Average time per image: {(time() - start) / len(image_paths):.2f} seconds")
    print(f"Inference completed. Latents saved to {output_dir}")

if __name__ == '__main__':
    # Example: python3.10 inference.py --img_path "sim_pushblock.png" --model_path "model_v2/" --prompt "pushblock"
    parser = argparse.ArgumentParser()
    parser.add_argument('--img_path', type=str)
    parser.add_argument('--model_path', type=str)
    parser.add_argument('--output_path', type=str, default='./')
    parser.add_argument('--prompt', type=str)
    parser.add_argument('--resolution', type=int, default=512)

    args = parser.parse_args()
    img_path = args.img_path
    model_path = args.model_path
    output_path = args.output_path
    prompt = args.prompt
    resolution = args.resolution

    #controlnet_infer(img_path, model_path, output_path, prompt)
    generate_latents(img_path, model_path, output_path, prompt, resolution)